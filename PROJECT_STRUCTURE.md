# ì´ë¯¸ì§€ ìº¡ì…”ë‹ í”„ë¡œì íŠ¸ êµ¬ì¡°

## ğŸ“ í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
final_image_captioning/
â”‚
â”œâ”€â”€ data/                      # ë°ì´í„° ê´€ë ¨
â”‚   â”œâ”€â”€ raw/                   # ì›ë³¸ ë°ì´í„° (Flickr8k)
â”‚   â”œâ”€â”€ processed/             # ì „ì²˜ë¦¬ëœ ë°ì´í„°
â”‚   â””â”€â”€ vocab/                 # ë‹¨ì–´ì¥ íŒŒì¼
â”‚
â”œâ”€â”€ datasets/                  # ë°ì´í„°ì…‹ ë¡œë”
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ flickr8k.py            # Flickr8k ë°ì´í„°ì…‹ ë¡œë” ë° ì „ì²˜ë¦¬
â”‚   â””â”€â”€ data/                  # ì‹¤ì œ ë°ì´í„° íŒŒì¼
â”‚       â”œâ”€â”€ Flickr8k_images/   # ì›ë³¸ ì´ë¯¸ì§€ íŒŒì¼ë“¤
â”‚       â””â”€â”€ captions_preprocessed/  # ì „ì²˜ë¦¬ëœ ìº¡ì…˜
â”‚           â”œâ”€â”€ captions_padded.csv
â”‚           â”œâ”€â”€ word2idx.json
â”‚           â””â”€â”€ idx2word.json
â”‚
â”œâ”€â”€ modules/                   # ëª¨ë¸ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ resnet_18.py           # ResNet-18 ì¸ì½”ë” êµ¬í˜„
â”‚   â”œâ”€â”€ encoder.py             # ì¸ì½”ë” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”‚   â”œâ”€â”€ decoder.py             # LSTM ë””ì½”ë” + Bahdanau Attention
â”‚   â”œâ”€â”€ preprocess.py          # ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤
â”‚   â”œâ”€â”€ evaluation.py          # í‰ê°€ ì§€í‘œ (BLEU, METEOR, ROUGE, CIDEr)
â”‚   â””â”€â”€ attention_viz.py       # Attention heatmap ì‹œê°í™”
â”‚
â”œâ”€â”€ models/                    # ì „ì²´ ëª¨ë¸ ì •ì˜
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ image_caption_model.py # Encoder-Decoder í†µí•© ëª¨ë¸
â”‚
â”œâ”€â”€ checkpoints/               # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
â”‚   â”œâ”€â”€ best_model.pth        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
â”‚   â”œâ”€â”€ checkpoint_epoch_*.pth # ì—í­ë³„ ì²´í¬í¬ì¸íŠ¸
â”‚   â””â”€â”€ attn_out/             # Attention heatmap ì €ì¥
â”‚       â””â”€â”€ epoch_*/          # ì—í­ë³„ ì‹œê°í™” ê²°ê³¼
â”‚
â”œâ”€â”€ outputs/                   # ê²°ê³¼ë¬¼ ì €ì¥
â”‚   â”œâ”€â”€ predictions/           # ìƒì„±ëœ ìº¡ì…˜
â”‚   â””â”€â”€ images/                # ê²°ê³¼ ì´ë¯¸ì§€
â”‚
â”œâ”€â”€ logs/                      # ë¡œê·¸ íŒŒì¼
â”‚
â”œâ”€â”€ notebooks/                 # Jupyter ë…¸íŠ¸ë¶ (ì„ íƒì‚¬í•­)
â”‚
â”œâ”€â”€ main.py                    # ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ visual.py                  # ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt           # Python íŒ¨í‚¤ì§€ ì˜ì¡´ì„±
â”œâ”€â”€ readme.md                  # í”„ë¡œì íŠ¸ ì„¤ëª…ì„œ
â””â”€â”€ PROJECT_STRUCTURE.md       # í”„ë¡œì íŠ¸ êµ¬ì¡° ë¬¸ì„œ (ë³¸ íŒŒì¼)
```

## ğŸ“‹ ì£¼ìš” íŒŒì¼ ìƒì„¸ ì„¤ëª…

### 1. ë©”ì¸ ì‹¤í–‰ íŒŒì¼

#### `main.py` - í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
**ì—­í• **: ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

**ì£¼ìš” í•¨ìˆ˜**:
- `train_one_epoch()`: í•œ ì—í­ í•™ìŠµ (forward, backward, optimizer.step í¬í•¨)
- `validate()`: ê²€ì¦ ë°ì´í„°ì…‹ í‰ê°€
- `evaluate_model()`: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í‰ê°€ (BLEU, METEOR, ROUGE, CIDEr)
- `save_checkpoint()` / `load_checkpoint()`: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë¡œë“œ
- `analyze_word_frequency()`: ë‹¨ì–´ ë¹ˆë„ ë¶„ì„
- `get_topk_predictions()`: Top-k ì˜ˆì¸¡ ì¶œë ¥ (ë””ë²„ê¹…ìš©)

**ì£¼ìš” í´ë˜ìŠ¤**:
- `LabelSmoothingCrossEntropy`: Label Smoothing ì ìš© ì†ì‹¤ í•¨ìˆ˜

**í•™ìŠµ ì„¤ì •**:
- `FAST_TEST = False`: ì‹¤ì œ í•™ìŠµ ëª¨ë“œ (25 ì—í­, ì „ì²´ ë°ì´í„°)
- `FAST_TEST = True`: ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (3 ì—í­, 1000ê°œ ìƒ˜í”Œ)
- ë°°ì¹˜ í¬ê¸°: 128
- í•™ìŠµë¥ : 0.001
- Label Smoothing: 0.1
- Gradient Clipping: 5.0

### 2. ëª¨ë¸ ì •ì˜

#### `models/image_caption_model.py` - í†µí•© ëª¨ë¸
**ì—­í• **: Encoderì™€ Decoderë¥¼ ì—°ê²°í•˜ëŠ” í†µí•© ëª¨ë¸

**ì£¼ìš” í´ë˜ìŠ¤**:
- `ImageCaptionModel`: Encoder-Decoder í†µí•© ëª¨ë¸

**ì£¼ìš” ë©”ì„œë“œ**:
- `forward()`: í•™ìŠµ ì‹œ forward pass (Teacher Forcing)
- `generate_caption()`: ì¶”ë¡  ì‹œ ìº¡ì…˜ ìƒì„±
  - Greedy search / Sampling / Top-k sampling ì§€ì›
  - ë°˜ë³µ ì–µì œ ë©”ì»¤ë‹ˆì¦˜ (repetition_penalty, no_repeat_ngram_size)
  - Attention ì •ë³´ ë°˜í™˜ ì˜µì…˜ (`return_attention=True`)

**ì…ë ¥/ì¶œë ¥**:
- ì…ë ¥: ì´ë¯¸ì§€ `[B, 3, 224, 224]`
- ì¶œë ¥: ë¡œì§“ `[B, seq_len, vocab_size]` (í•™ìŠµ) ë˜ëŠ” ìº¡ì…˜ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ (ì¶”ë¡ )

#### `modules/resnet_18.py` - ResNet-18 ì¸ì½”ë”
**ì—­í• **: ì´ë¯¸ì§€ì—ì„œ íŠ¹ì§• ì¶”ì¶œ

**ì£¼ìš” í´ë˜ìŠ¤**:
- `Stem`: ì´ˆê¸° ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
- `BasicBlock`: ResNet ê¸°ë³¸ ë¸”ë¡
- `ResNet`: ResNet-18 ì „ì²´ êµ¬ì¡°

**ì¶œë ¥ í˜•íƒœ**:
- `embed_size`ê°€ ì£¼ì–´ì§€ë©´: `(global_feat, spatial_feat, (H, W))`
  - `global_feat`: `[B, embed_size]` - ì „ì²´ ì´ë¯¸ì§€ íŠ¹ì§•
  - `spatial_feat`: `[B, H*W, embed_size]` - ê³µê°„ì  íŠ¹ì§• (Attentionìš©)
  - `(H, W)`: íŠ¹ì§• ë§µ í¬ê¸° (ì˜ˆ: (7, 7))

**êµ¬ì¡°**:
- Stem â†’ Layer1 â†’ Layer2 â†’ Layer3 â†’ Layer4
- Global feature: AdaptiveAvgPool2d + Linear projection
- Spatial feature: Layer4 ì¶œë ¥ì„ flatten + Linear projection

#### `modules/decoder.py` - LSTM ë””ì½”ë” + Attention
**ì—­í• **: ì´ë¯¸ì§€ íŠ¹ì§•ìœ¼ë¡œë¶€í„° ìº¡ì…˜ ìƒì„±

**ì£¼ìš” í´ë˜ìŠ¤**:
- `BahdanauAttention`: Additive Attention ë©”ì»¤ë‹ˆì¦˜
  - ì…ë ¥: `encoder_out [B, P, E]`, `decoder_h [B, H]`
  - ì¶œë ¥: `context [B, E]`, `alpha [B, P]`
  - Temperature scalingìœ¼ë¡œ attentionì„ ë” ë‚ ì¹´ë¡­ê²Œ ë§Œë“¦
- `CaptionDecoder`: ë‹¤ì¸µ LSTM ë””ì½”ë”
  - Embedding â†’ Attention â†’ LSTM (2 layers) â†’ Linear
  - Contextì™€ word embeddingì„ concatí•˜ì—¬ LSTM ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
  - í•™ìŠµ ê°€ëŠ¥í•œ context ê°€ì¤‘ì¹˜ (`context_weight`)
- `LSTMCell`: ì»¤ìŠ¤í…€ LSTM ì…€ êµ¬í˜„

**ì£¼ìš” ë©”ì„œë“œ**:
- `init_hidden_state()`: Global featureë¡œ ì´ˆê¸° hidden state ê³„ì‚°
- `step()`: í•œ ë‹¨ê³„ ë””ì½”ë”© (ì¶”ë¡ ìš©)
- `forward()`: ì „ì²´ ì‹œí€€ìŠ¤ ë””ì½”ë”© (í•™ìŠµìš©, Teacher Forcing)

**ì…ë ¥/ì¶œë ¥**:
- ì…ë ¥: `features [B, E]`, `captions [B, T]`, `encoder_out [B, P, E]` (optional)
- ì¶œë ¥: `outputs [B, T, vocab_size]`, `alphas [B, T, P]` (optional)

### 3. ë°ì´í„°ì…‹

#### `datasets/flickr8k.py` - Flickr8k ë°ì´í„°ì…‹ ë¡œë”
**ì—­í• **: Flickr8k ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬

**ì£¼ìš” í´ë˜ìŠ¤**:
- `Flickr8kDataset`: ì „ì²˜ë¦¬ìš© í´ë˜ìŠ¤
  - `load_captions_to_df()`: ìº¡ì…˜ ì „ì²˜ë¦¬ ë° ë‹¨ì–´ì¥ ìƒì„±
  - `preprocess_image()`: ì´ë¯¸ì§€ ì „ì²˜ë¦¬
- `Flickr8kImageCaptionDataset`: PyTorch Dataset
  - `__getitem__()`: ì´ë¯¸ì§€ì™€ ìº¡ì…˜ í…ì„œ ë°˜í™˜
  - ìë™ train/val/test ë¶„í•  (80/10/10)
- `Flickr8kImageOnlyDataset`: ì´ë¯¸ì§€ë§Œ ë¡œë“œí•˜ëŠ” ë°ì´í„°ì…‹

**ì „ì²˜ë¦¬ ê³¼ì •**:
1. í…ìŠ¤íŠ¸ ì •ì œ: ì†Œë¬¸ì ë³€í™˜, íŠ¹ìˆ˜ë¬¸ì ì œê±°
2. í† í°í™”: ê³µë°± ê¸°ì¤€ ë¶„ë¦¬
3. ë‹¨ì–´ì¥ êµ¬ì¶•: íŠ¹ìˆ˜ í† í° (`<pad>`, `<start>`, `<end>`, `<unk>`) + ë‹¨ì–´ë“¤
4. ì¸ë±ìŠ¤ ë³€í™˜: ë‹¨ì–´ â†’ ì¸ë±ìŠ¤
5. íŒ¨ë”©: ìµœëŒ€ ê¸¸ì´ 20ìœ¼ë¡œ íŒ¨ë”©

**ì¶œë ¥ íŒŒì¼**:
- `captions_padded.csv`: íŒ¨ë”©ëœ ìº¡ì…˜
- `word2idx.json`: ë‹¨ì–´ â†’ ì¸ë±ìŠ¤ ë§¤í•‘
- `idx2word.json`: ì¸ë±ìŠ¤ â†’ ë‹¨ì–´ ë§¤í•‘

### 4. í‰ê°€ ë° ì‹œê°í™”

#### `modules/evaluation.py` - í‰ê°€ ì§€í‘œ
**ì—­í• **: ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œ ê³„ì‚°

**ì£¼ìš” í•¨ìˆ˜**:
- `calculate_bleu()`: BLEU-1, BLEU-2, BLEU-3, BLEU-4 ì ìˆ˜
- `calculate_meteor()`: METEOR ì ìˆ˜ (ë™ì˜ì–´ ê³ ë ¤)
- `calculate_rouge()`: ROUGE-L ì ìˆ˜
- `calculate_cider()`: CIDEr ì ìˆ˜ (ì´ë¯¸ì§€ ìº¡ì…”ë‹ íŠ¹í™”)

**ì‚¬ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬**:
- NLTK: BLEU, METEOR ê³„ì‚°
- pycocotools: CIDEr ê³„ì‚°

#### `modules/attention_viz.py` - Attention ì‹œê°í™”
**ì—­í• **: Attention heatmapì„ ì´ë¯¸ì§€ì— ì˜¤ë²„ë ˆì´í•˜ì—¬ ì €ì¥

**ì£¼ìš” í•¨ìˆ˜**:
- `save_attention_overlays()`: ê° ë‹¨ì–´ë³„ attention heatmap ì €ì¥
  - ì…ë ¥: PIL Image, ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸, alpha ë¦¬ìŠ¤íŠ¸, spatial í¬ê¸°
  - ì¶œë ¥: ê° ë‹¨ì–´ë³„ heatmap ì´ë¯¸ì§€ íŒŒì¼

**ì €ì¥ í˜•ì‹**:
- íŒŒì¼ëª…: `{prefix}_{step}_{word}.png`
- ì˜ˆ: `1000268201_693b08cb0e_0_a.png`, `1000268201_693b08cb0e_1_dog.png`

### 5. ìœ í‹¸ë¦¬í‹°

#### `modules/encoder.py` - ì¸ì½”ë” ìœ í‹¸ë¦¬í‹°
**ì—­í• **: ì¸ì½”ë” ê´€ë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜

**ì£¼ìš” í•¨ìˆ˜**:
- `encode_images()`: ë°°ì¹˜ ì´ë¯¸ì§€ë¥¼ íŠ¹ì§• ë²¡í„°ë¡œ ì¸ì½”ë”©

#### `modules/preprocess.py` - ì „ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
**ì—­í• **: ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜

## ğŸ”„ ì „ì²´ ë°ì´í„° íë¦„ (End-to-End Pipeline)

### Phase 1: ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬

```
ì›ë³¸ ë°ì´í„° (Flickr8k)
    â†“
datasets/flickr8k.py::Flickr8kDataset.load_captions_to_df()
    â†“
í…ìŠ¤íŠ¸ ì •ì œ (ì†Œë¬¸ì, íŠ¹ìˆ˜ë¬¸ì ì œê±°)
    â†“
í† í°í™” (ê³µë°± ê¸°ì¤€ ë¶„ë¦¬)
    â†“
ë‹¨ì–´ì¥ êµ¬ì¶• (word2idx, idx2word)
    â†“
ì¸ë±ìŠ¤ ë³€í™˜ + íŒ¨ë”© (ìµœëŒ€ ê¸¸ì´ 20)
    â†“
ì €ì¥: captions_padded.csv, word2idx.json, idx2word.json
```

**íŠ¹ìˆ˜ í† í°**:
- `<pad>` (idx=0): íŒ¨ë”© í† í°
- `<start>` (idx=1): ì‹œì‘ í† í°
- `<end>` (idx=2): ì¢…ë£Œ í† í°
- `<unk>` (idx=3): ì•Œ ìˆ˜ ì—†ëŠ” ë‹¨ì–´

### Phase 2: ë°ì´í„°ì…‹ ë¡œë”©

```
datasets/flickr8k.py::Flickr8kImageCaptionDataset
    â†“
ì´ë¯¸ì§€ ë¡œë“œ (PIL Image)
    â†“
Transform ì ìš© (Resize 224x224, ToTensor, Normalize)
    â†“
ìº¡ì…˜ í…ì„œ ë³€í™˜ (LongTensor)
    â†“
DataLoader (ë°°ì¹˜ ìƒì„±)
    â†“
ì¶œë ¥: (images [B, 3, 224, 224], captions [B, T])
```

**ë°ì´í„° ë¶„í• **:
- í•™ìŠµ: 80%
- ê²€ì¦: 10%
- í…ŒìŠ¤íŠ¸: 10%

### Phase 3: ëª¨ë¸ ì•„í‚¤í…ì²˜ (Encoder-Decoder with Attention)

```
ì…ë ¥ ì´ë¯¸ì§€ [B, 3, 224, 224]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ResNet-18 Encoder                  â”‚
â”‚  (modules/resnet_18.py)             â”‚
â”‚  - Stem â†’ Layer1-4                  â”‚
â”‚  - Global feature: [B, embed_size]  â”‚
â”‚  - Spatial feature: [B, H*W, E]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
(global_feat, spatial_feat, (H, W))
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CaptionDecoder                     â”‚
â”‚  (modules/decoder.py)               â”‚
â”‚  - Embedding                        â”‚
â”‚  - BahdanauAttention                â”‚
â”‚  - Multi-layer LSTM (2 layers)      â”‚
â”‚  - Linear Output                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
ì¶œë ¥ ë¡œì§“ [B, seq_len, vocab_size]
```

**Attention ë©”ì»¤ë‹ˆì¦˜**:
1. Encoderì˜ spatial feature `[B, P, E]`ì™€ Decoderì˜ hidden state `[B, H]`ë¥¼ ì…ë ¥
2. Attention score ê³„ì‚°: `tanh(W_enc * encoder_out + W_dec * decoder_h)`
3. Softmaxë¡œ attention weight `alpha [B, P]` ê³„ì‚°
4. Weighted sumìœ¼ë¡œ context `[B, E]` ìƒì„±
5. Contextì™€ word embeddingì„ concatí•˜ì—¬ LSTM ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©

### Phase 4: í•™ìŠµ ê³¼ì • (main.py)

```python
for epoch in range(num_epochs):
    # 1. í•™ìŠµ
    for batch in train_loader:
        images, captions = batch  # [B, 3, 224, 224], [B, T]
        
        # Forward Pass
        outputs = model(images, captions)  # [B, T, vocab_size]
        targets = captions[:, 1:]  # Teacher Forcing: <start> ì œì™¸
        
        # Loss ê³„ì‚° (Label Smoothing ì ìš©)
        outputs_flat = outputs.reshape(-1, vocab_size)
        targets_flat = targets.reshape(-1)
        loss = criterion(outputs_flat, targets_flat)
        
        # Backward Pass
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
        optimizer.step()
    
    # 2. ê²€ì¦
    val_loss = validate(model, val_loader, ...)
    
    # 3. Attention ì‹œê°í™” (ë§¤ ì—í­ë§ˆë‹¤)
    sample_image = val_dataset[random_idx]
    captions, attn_info = model.generate_caption(
        sample_image, idx2word, 
        return_attention=True,
        repetition_penalty=1.5,
        no_repeat_ngram_size=3
    )
    save_attention_overlays(...)
    
    # 4. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    if val_loss < best_val_loss:
        save_checkpoint(..., is_best=True)
```

**í•™ìŠµ ì „ëµ**:
- **Encoder í™œì„±í™”**: Encoder íŒŒë¼ë¯¸í„°ë„ í•¨ê»˜ í•™ìŠµ (`requires_grad=True`)
- **Label Smoothing**: ê³¼ì í•© ë°©ì§€ (smoothing=0.1)
- **Gradient Clipping**: ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ë°©ì§€ (max_norm=5.0)
- **Teacher Forcing**: í•™ìŠµ ì‹œ ì •ë‹µ ìº¡ì…˜ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©

### Phase 5: ì¶”ë¡  ê³¼ì • (generate_caption)

```
ìƒˆë¡œìš´ ì´ë¯¸ì§€ [1, 3, 224, 224]
    â†“
Encoder â†’ (global_feat, spatial_feat, (H, W))
    â†“
ì´ˆê¸° hidden state ê³„ì‚° (global_feat ê¸°ë°˜)
    â†“
for step in range(max_length):
    í˜„ì¬ ë‹¨ì–´ embedding
        â†“
    Attention (spatial_feat, hidden_state) â†’ context
        â†“
    [word_embed; context] â†’ LSTM â†’ logits
        â†“
    ë°˜ë³µ ì–µì œ ì ìš© (repetition_penalty, no_repeat_ngram)
        â†“
    Greedy/Samplingìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì–´ ì„ íƒ
        â†“
    <end> í† í°ì´ë©´ ì¢…ë£Œ
    â†“
í† í° ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤
    â†“
Vocabulary.decode() â†’ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
    â†“
ìƒì„±ëœ ìº¡ì…˜ ë¬¸ìì—´
```

**ë°˜ë³µ ì–µì œ ë©”ì»¤ë‹ˆì¦˜**:
1. **N-gram ë°˜ë³µ ë°©ì§€**: ìµœê·¼ Nê°œ ë‹¨ì–´ì™€ ë™ì¼í•œ ë‹¨ì–´ì˜ logitì„ ë‚®ì¶¤
2. **ì§ì „ í† í° ì–µì œ**: ì§ì „ì— ìƒì„±ëœ ë‹¨ì–´ì˜ logitì„ ë” ê°•í•˜ê²Œ ë‚®ì¶¤
3. **Repetition Penalty**: ë°˜ë³µëœ í† í°ì˜ logitì„ `repetition_penalty`ë¡œ ë‚˜ëˆ”

### Phase 6: í‰ê°€ (evaluate_model)

```
í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
    â†“
ëª¨ë¸ë¡œ ìº¡ì…˜ ìƒì„± (Greedy search)
    â†“
ìƒì„±ëœ ìº¡ì…˜ vs ì°¸ì¡° ìº¡ì…˜
    â†“
modules/evaluation.py
    â†“
BLEU-1, BLEU-2, BLEU-3, BLEU-4
METEOR
ROUGE-L
CIDEr
```

## 3. ëª¨ë“ˆ ê°„ ì˜ì¡´ì„± ê´€ê³„

```
main.py
    â”œâ”€â”€ models/image_caption_model.py
    â”‚   â”œâ”€â”€ modules/resnet_18.py (ResNet)
    â”‚   â””â”€â”€ modules/decoder.py (CaptionDecoder)
    â”‚       â””â”€â”€ BahdanauAttention
    â”‚
    â”œâ”€â”€ datasets/flickr8k.py
    â”‚   â””â”€â”€ Flickr8kImageCaptionDataset
    â”‚
    â”œâ”€â”€ modules/evaluation.py
    â”‚   â””â”€â”€ calculate_bleu, calculate_meteor, ...
    â”‚
    â””â”€â”€ modules/attention_viz.py
        â””â”€â”€ save_attention_overlays
```

## 4. íŒŒì¼ë³„ ì—­í•  ìš”ì•½

| íŒŒì¼/ëª¨ë“ˆ | ì—­í•  | ì£¼ìš” í´ë˜ìŠ¤/í•¨ìˆ˜ |
|----------|------|-----------------|
| `main.py` | í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ | `train_one_epoch()`, `validate()`, `evaluate_model()`, `LabelSmoothingCrossEntropy` |
| `models/image_caption_model.py` | í†µí•© ëª¨ë¸ | `ImageCaptionModel` |
| `modules/resnet_18.py` | ì´ë¯¸ì§€ ì¸ì½”ë” | `ResNet`, `Stem`, `BasicBlock` |
| `modules/decoder.py` | ìº¡ì…˜ ë””ì½”ë” | `CaptionDecoder`, `BahdanauAttention`, `LSTMCell` |
| `modules/encoder.py` | ì¸ì½”ë” ìœ í‹¸ë¦¬í‹° | `encode_images()` |
| `modules/evaluation.py` | í‰ê°€ ì§€í‘œ | `calculate_bleu()`, `calculate_meteor()`, `calculate_rouge()`, `calculate_cider()` |
| `modules/attention_viz.py` | Attention ì‹œê°í™” | `save_attention_overlays()` |
| `modules/preprocess.py` | ì „ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° | ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ |
| `datasets/flickr8k.py` | ë°ì´í„°ì…‹ ë¡œë” | `Flickr8kDataset`, `Flickr8kImageCaptionDataset` |

## 5. ì‹¤í–‰ íë¦„ ì˜ˆì‹œ

### í•™ìŠµ ì‹¤í–‰

```python
# 1. ë°ì´í„° ì „ì²˜ë¦¬ (ìµœì´ˆ 1íšŒë§Œ)
dataset = Flickr8kDataset()
dataset.load_captions_to_df()  # ë‹¨ì–´ì¥ êµ¬ì¶•

# 2. ë°ì´í„°ì…‹ ìƒì„±
train_dataset = Flickr8kImageCaptionDataset(
    image_dir="datasets/data/Flickr8k_images",
    captions_file="datasets/data/captions_preprocessed/captions_padded.csv",
    transform=transform,
    split="train"
)

# 3. ëª¨ë¸ ìƒì„±
encoder = ResNet(embed_size=256)
decoder = CaptionDecoder(
    embed_size=256,
    hidden_size=512,
    vocab_size=vocab_size,
    num_layers=2,
    dropout=0.1
)
model = ImageCaptionModel(encoder, decoder, vocab_size)

# 4. í•™ìŠµ
python main.py
```

### ì¶”ë¡  ì‹¤í–‰

```python
# ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
checkpoint = torch.load("checkpoints/best_model.pth")
model.load_state_dict(checkpoint['model_state_dict'])

# ìº¡ì…˜ ìƒì„±
image = Image.open("test_image.jpg")
transform = transforms.Compose([...])
image_tensor = transform(image).unsqueeze(0)

caption, attn_info = model.generate_caption(
    image_tensor,
    idx2word,
    max_length=20,
    return_attention=True,
    repetition_penalty=1.5,
    no_repeat_ngram_size=3
)

print(caption[0])  # "a dog runs in the grass"
```

## 6. í•µì‹¬ ì„¤ê³„ ì›ì¹™

### ëª¨ë“ˆí™”
- **Encoder/Decoder ë¶„ë¦¬**: ê°ê° ë…ë¦½ì ìœ¼ë¡œ ìˆ˜ì • ê°€ëŠ¥
- **Attention ë©”ì»¤ë‹ˆì¦˜ ë¶„ë¦¬**: `BahdanauAttention` í´ë˜ìŠ¤ë¡œ ë…ë¦½ êµ¬í˜„
- **í‰ê°€ ì§€í‘œ ë¶„ë¦¬**: `evaluation.py`ì— ëª¨ë“  í‰ê°€ í•¨ìˆ˜ ì§‘ì¤‘

### ì¬ì‚¬ìš©ì„±
- **ë°ì´í„°ì…‹ í´ë˜ìŠ¤**: ì—¬ëŸ¬ ë°ì´í„°ì…‹ì— ì ìš© ê°€ëŠ¥í•œ êµ¬ì¡°
- **ëª¨ë¸ ì•„í‚¤í…ì²˜**: Encoder/Decoderë¥¼ êµì²´í•˜ì—¬ ë‹¤ë¥¸ ëª¨ë¸ êµ¬ì„± ê°€ëŠ¥

### í™•ì¥ì„±
- **í•˜ì´í¼íŒŒë¼ë¯¸í„°**: `main.py`ì—ì„œ ì¤‘ì•™ ê´€ë¦¬
- **ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œ**: í•™ìŠµ ì¤‘ë‹¨ í›„ ì¬ê°œ ê°€ëŠ¥
- **Attention ì‹œê°í™”**: ê° ë‹¨ì–´ë³„ attention ë¶„ì„ ê°€ëŠ¥

### ì¶”ì ì„±
- **ë¡œê¹…**: í•™ìŠµ/ê²€ì¦ ì†ì‹¤ ì‹¤ì‹œê°„ ì¶œë ¥
- **ì²´í¬í¬ì¸íŠ¸**: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìë™ ì €ì¥
- **Attention heatmap**: ê° ì—í­ë§ˆë‹¤ ì‹œê°í™” ê²°ê³¼ ì €ì¥

## 7. ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

### ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
- `embed_size`: 256 (ì¸ì½”ë” ì¶œë ¥ ì°¨ì›)
- `hidden_size`: 512 (LSTM hidden ì°¨ì›)
- `num_layers`: 2 (LSTM ë ˆì´ì–´ ìˆ˜)
- `dropout`: 0.1
- `vocab_size`: ë‹¨ì–´ì¥ í¬ê¸° (ë™ì )

### í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
- `batch_size`: 128
- `num_epochs`: 25 (ì‹¤ì œ í•™ìŠµ) / 3 (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
- `learning_rate`: 0.001
- `weight_decay`: 0.0001
- `gradient_clip`: 5.0
- `label_smoothing`: 0.1

### ì¶”ë¡  í•˜ì´í¼íŒŒë¼ë¯¸í„°
- `max_length`: 20 (ìµœëŒ€ ìº¡ì…˜ ê¸¸ì´)
- `repetition_penalty`: 1.5 (ë°˜ë³µ ì–µì œ ê°•ë„)
- `no_repeat_ngram_size`: 3 (N-gram ë°˜ë³µ ë°©ì§€)
- `temperature`: 1.0 (ìƒ˜í”Œë§ ì˜¨ë„)
- `beam_size`: 1 (í˜„ì¬ëŠ” Greedyë§Œ ì§€ì›)

## 8. ë°ì´í„° êµ¬ì¡°

### ì…ë ¥ ë°ì´í„°
- **ì´ë¯¸ì§€**: RGB ì´ë¯¸ì§€, 224x224 í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
- **ìº¡ì…˜**: ìµœëŒ€ ê¸¸ì´ 20, íŒ¨ë”© í¬í•¨

### ì¶œë ¥ ë°ì´í„°
- **ì²´í¬í¬ì¸íŠ¸**: ëª¨ë¸ ê°€ì¤‘ì¹˜, ì˜µí‹°ë§ˆì´ì € ìƒíƒœ, ì—í­ ë²ˆí˜¸, ì†ì‹¤
- **Attention heatmap**: ê° ë‹¨ì–´ë³„ attention ê°€ì¤‘ì¹˜ë¥¼ ì´ë¯¸ì§€ì— ì˜¤ë²„ë ˆì´
- **í‰ê°€ ê²°ê³¼**: BLEU, METEOR, ROUGE, CIDEr ì ìˆ˜

## 9. ì£¼ì˜ì‚¬í•­ ë° ì œí•œì‚¬í•­

### í˜„ì¬ êµ¬í˜„ëœ ê¸°ëŠ¥
- âœ… ResNet-18 ê¸°ë°˜ ì¸ì½”ë”
- âœ… Bahdanau Attention ë©”ì»¤ë‹ˆì¦˜
- âœ… ë‹¤ì¸µ LSTM ë””ì½”ë”
- âœ… Label Smoothing
- âœ… ë°˜ë³µ ì–µì œ ë©”ì»¤ë‹ˆì¦˜
- âœ… Attention ì‹œê°í™”
- âœ… ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œ (BLEU, METEOR, ROUGE, CIDEr)

### ë¯¸êµ¬í˜„ ê¸°ëŠ¥
- âŒ Beam Search (í˜„ì¬ëŠ” Greedyë§Œ ì§€ì›)
- âŒ Transformer ê¸°ë°˜ ë””ì½”ë”
- âŒ ë‹¤ë¥¸ ì¸ì½”ë” (VGG, EfficientNet ë“±)
- âŒ MS COCO ë°ì´í„°ì…‹ ì§€ì›

### ì„±ëŠ¥ ìµœì í™”
- GPU ì‚¬ìš© ì‹œ `pin_memory=True`ë¡œ ì„¤ì •
- `num_workers=4`ë¡œ ë°ì´í„° ë¡œë”© ë³‘ë ¬í™”
- Gradient clippingìœ¼ë¡œ í•™ìŠµ ì•ˆì •í™”

## 10. ë””ë²„ê¹… ë° ëª¨ë‹ˆí„°ë§

### í•™ìŠµ ëª¨ë‹ˆí„°ë§
- **Top-k ì˜ˆì¸¡ ì¶œë ¥**: ê° ë°°ì¹˜ë§ˆë‹¤ ìƒìœ„ kê°œ ì˜ˆì¸¡ ë‹¨ì–´ ì¶œë ¥
- **ë‹¨ì–´ ë¹ˆë„ ë¶„ì„**: ë°ì´í„°ì…‹ì˜ ë‹¨ì–´ ë¹ˆë„ í†µê³„
- **ì†ì‹¤ ì¶”ì **: í•™ìŠµ/ê²€ì¦ ì†ì‹¤ ì‹¤ì‹œê°„ ì¶œë ¥

### Attention ë¶„ì„
- **Heatmap ì €ì¥**: ê° ì—í­ë§ˆë‹¤ ìƒ˜í”Œ ì´ë¯¸ì§€ì˜ attention heatmap ì €ì¥
- **ë‹¨ì–´ë³„ Attention**: ê° ë‹¨ì–´ ìƒì„± ì‹œ ëª¨ë¸ì´ ì£¼ëª©í•œ ì´ë¯¸ì§€ ì˜ì—­ ì‹œê°í™”

### ë¬¸ì œ ì§„ë‹¨
- **ë¶•ê´´ ê°ì§€**: ê°™ì€ ë‹¨ì–´ê°€ ì—°ì†ìœ¼ë¡œ 3ë²ˆ ì´ìƒ ë°˜ë³µë˜ë©´ ê²½ê³ 
- **Encoder í•™ìŠµ í™•ì¸**: Encoder íŒŒë¼ë¯¸í„°ì˜ ê·¸ë˜ë””ì–¸íŠ¸ í™•ì¸
- **íŒŒì¼ ê²½ë¡œ ê²€ì¦**: ì´ë¯¸ì§€ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
